{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d593fd4-5dd1-4ac7-ab72-98b4d5743e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "import spacy\n",
    "nlp_id = spacy.blank('id')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117a480-cd88-4cde-9903-81c6e19732a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/News.csv\")\n",
    "df.isnull().sum()\n",
    "# Dropping the rows having None values\n",
    "df = df.dropna(subset=['content'])\n",
    "# Resetting the indices\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de8cb0-751a-40d5-87f7-d995e2bef7bc",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abac6c-f4ed-426b-831e-927e384cd3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stem(token_DocID):\n",
    "    new_token = []\n",
    "    for to_ in token_DocID:\n",
    "        new_token.append( ( stemmer.stem(to_) ))\n",
    "    return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f2941-6c1c-41ea-80ec-547d81d77c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_tokenization(contents):\n",
    "    list_tokens_from_docs = []\n",
    "    for content in contents:\n",
    "        nlp_contents = nlp_id(content.lower())\n",
    "        clean_token = []\n",
    "        for token_of_nlp_contents in nlp_contents:\n",
    "            # remove punctuation & remove stopword\n",
    "            if not token_of_nlp_contents.is_digit and not token_of_nlp_contents.is_punct \\\n",
    "                and not token_of_nlp_contents.is_stop:\n",
    "                # clean_token.append(str(token_of_nlp_contents))\n",
    "                clean_token.append(token_of_nlp_contents)\n",
    "                # print(token_of_nlp_contents)\n",
    "        list_tokens_from_docs.append(clean_token)\n",
    "    return list_tokens_from_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be5679-a3df-4caa-84a3-79f5eff467d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stem = sentence_stem(df['content'][0:30])\n",
    "nlp_tokens = nlp_tokenization(nlp_stem)\n",
    "# nlp_tokens = nlp_tokenization(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9407d-6165-4a83-8734-1e19b6800f53",
   "metadata": {},
   "source": [
    "## Pairing Term and Document ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011c100-8e78-4235-8c69-ec2755bca1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_token_docID(nlp_tokens):\n",
    "    doc_list_ofToken_DocID = []\n",
    "    # index_docID\n",
    "    for docID, doc_list_T in enumerate(nlp_tokens):\n",
    "        # doc_list_ofToken_DocID_post = []\n",
    "        for doc_list in doc_list_T:\n",
    "            # doc_list_ofToken_DocID_post.append((doc_list, docID+1))\n",
    "            # doc_list_ofToken_DocID.append((doc_list, docID+1))\n",
    "            doc_list_ofToken_DocID.append((doc_list.text, docID+1))\n",
    "    return doc_list_ofToken_DocID\n",
    "\n",
    "def remove_space(nlp_tokens_tup):\n",
    "    new_tokens = []\n",
    "    for i in nlp_tokens_tup:\n",
    "        if not i[0].isspace():\n",
    "            new_tokens.append(i)\n",
    "    return new_tokens\n",
    "\n",
    "def sort_token_docID(tup):\n",
    "    return(sorted(tup, key = lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c1662-c70c-469f-948d-18564737d37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp_tokens_docId = pair_token_docID(nlp_tokens)\n",
    "nlp_tokens_docId_clean_space = remove_space(nlp_tokens_docId)\n",
    "nlp_tokens_docId_sorted = sort_token_docID(nlp_tokens_docId_clean_space)\n",
    "nlp_tokens_docId_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4f0b1-6a7f-4494-ae8c-4b4e32e9b65b",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf0080-5a28-4054-911e-09a7700b516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a9b0f-7771-428f-9c4d-b944d7dc8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(tokens_docID):\n",
    "    vocab = {}\n",
    "    for token, docId in tokens_docID:\n",
    "        if not token in vocab:\n",
    "            vocab[token] = deque()\n",
    "            vocab[token].append(docId)\n",
    "        else:\n",
    "            temp_post_list = vocab[token]\n",
    "            temp_post_list.append(docId)\n",
    "            # prevent duplication\n",
    "            temp_post_list = set(temp_post_list)\n",
    "            # back to linkedlist\n",
    "            temp_post_list = deque(temp_post_list)\n",
    "            vocab[token] = temp_post_list\n",
    "    vocab_freq = {}\n",
    "    # pairing term and doc frequency\n",
    "    for key, val in vocab.items():\n",
    "        vocab_freq[(key, len(val))] = val\n",
    "    return vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d9d6a-9879-4f6c-991a-36b8850d44ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_inverted_index = inverted_index(nlp_tokens_docId_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34d9e2-3ffc-4436-8629-828ed6642063",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inverted_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "type_exp01",
   "language": "python",
   "name": "type_exp01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
